{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Challenge.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtR-MPPUqEn4",
        "colab_type": "code",
        "outputId": "d5fd5fe3-878d-41c8-e4a8-1fd9fbcf727d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import os\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "from numpy import newaxis\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "!pip install geohash2\n",
        "import geohash2 as gh\n",
        "from math import floor\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting geohash2\n",
            "  Downloading https://files.pythonhosted.org/packages/3e/0d/c40ea785cc5fa33c2f1e796ee02e763207140542d7aae2c6ea413358c092/geohash2-1.1.tar.gz\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.6/dist-packages (from geohash2) (0.14)\n",
            "Building wheels for collected packages: geohash2\n",
            "  Building wheel for geohash2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/63/0d/560a1741fa3f0ab897105cddb33f21f38c3330e4a57ea75db6\n",
            "Successfully built geohash2\n",
            "Installing collected packages: geohash2\n",
            "Successfully installed geohash2-1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cR0dOcf3Zsp",
        "colab_type": "code",
        "outputId": "890d0b8a-2610-412f-a202-5b5d8d209550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 26.0MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 2.8MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ4l79Ri3_j2",
        "colab_type": "code",
        "outputId": "922a96ee-caf9-4f7e-c542-d42ebfbb1473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        }
      },
      "source": [
        "linkts ='https://drive.google.com/open?id=1uJZxoU_TlhJo1ivP0T6ikH5kqORGnnrD'\n",
        "linktr = \"https://drive.google.com/open?id=1oqmqxb72zPnDLer2UqqRaAbCVggQPOB7\"\n",
        "fluff, tr_link = linktr.split('=')\n",
        "fluff,ts_link = linkts.split('=')\n",
        "downloaded_tr = drive.CreateFile({'id':tr_link}) \n",
        "downloaded_tr.GetContentFile('training.csv')  \n",
        "downloaded_ts = drive.CreateFile({'id':ts_link}) \n",
        "downloaded_ts.GetContentFile('test.csv')  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0617 14:34:44.471776 140675626592128 __init__.py:44] file_cache is unavailable when using oauth2client >= 4.0.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
            "    from google.appengine.api import memcache\n",
            "ModuleNotFoundError: No module named 'google.appengine'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
            "    from oauth2client.contrib.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
            "    from oauth2client.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
            "    from . import file_cache\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    'file_cache is unavailable when using oauth2client >= 4.0.0')\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZTc4l6xmtD0",
        "colab_type": "text"
      },
      "source": [
        "# Traffic Demand Prediction\n",
        "</br>Dear Judge, I have written the notebook with the concept of walkthrough that serves as a journey how I approach the problem. This competition has been interesting and I have learned alot. Below is my journey to slove the problem.\n",
        "\n",
        "</br>The Initial analysis of the traffic demand prediction is to understand the excel file given (training data). Excel csv file are given in the format of *geohash6*, *day*, *time* and *demand*. I proceed to check what are the maximum value of given column within the excel file. There are few things that I have incurred from the given training.csv file :\n",
        "\n",
        "\n",
        "1.   The maximum and the mininum value in the column day is between 1 to 61. The timestamp of the data given are in every 15 minutes. This gives me around 5856 recording points to work with. Number of timestamp given in Time Series data is very important for Data Scientist to think how to work with given data in predictive modelling.\n",
        "\n",
        "2.   With known unique value within **day** and **timestamp** column, I have found out that there's overlapping data in terms of each given timestamp, the only difference for data with same timestep is the demand and the geohash. This prompt me to model the problem of traffic prediction into different prespective.\n",
        "\n",
        "3.   The geohash6 column represent information of latitude and longitude that is encoded using a geohash. Location information can be easily retrieve by using geohash2 library in python to decode and obtain relevant information.\n",
        "\n",
        "This notebook has assumed that test data are given in the same format as training data where the time and day are not sorted out and location information given are only in geohash and normalized demand.\n",
        "\n",
        "\n",
        "## WARNING!!! \n",
        "Before proceed with the notebook, please make sure that the 'training.csv' and 'test.csv' are in the same folder as this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpkhlTpJu51D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "from numpy import newaxis\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import geohash2 as gh\n",
        "from math import floor\n",
        "from tqdm import tqdm\n",
        "\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.callbacks import *\n",
        "from keras.utils import *\n",
        "from keras.utils.generic_utils import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk-CDp9hvV6i",
        "colab_type": "text"
      },
      "source": [
        "I have started by importing all the libraries I need to process and train a predictive model. Tester of the notebook might need to install geohash2 by using 'pip install geohash2' and install keras by using 'pip install keras'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqhTvD-33O5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_csv(csvfile):\n",
        "  \"\"\"\n",
        "  INPUT : csvfile\n",
        "  OUTPUT : sorted csvfile based on time and day with information location decoded\n",
        "  FUNCTION : This function take in csvfile and decode the geohash location, \n",
        "              add additional 2 column of 'latitude' and 'longitude' to the existing csvfile \n",
        "              and sorted the csv file based on the day and then the time columns\n",
        "  \"\"\"\n",
        "  if not(os.path.isfile('{}_transform.csv'.format(csvfile.split('.')[0]))):\n",
        "      df_total = pd.read_csv(csvfile)\n",
        "      coordinate_list = df_total.apply(lambda x:gh.decode_exactly(x.geohash6), axis=1).tolist()\n",
        "      df_total['latitude']= [x[0] for x in coordinate_list]\n",
        "      df_total['longitude']= [x[1] for x in coordinate_list]\n",
        "      df_total['time'] = df_total.apply(lambda x:dt.datetime.strptime(x.timestamp, '%H:%M').time(), axis=1)\n",
        "      df_total = df_total.sort_values(by=['day','time','latitude','longitude'])\n",
        "      n_days = len(df_total['day'].unique().tolist())\n",
        "      timestamps = df_total.sort_values(by=['time']).time.unique().tolist()\n",
        "      timestamps_dict = dict()\n",
        "      for ii in range(len(timestamps)):\n",
        "          timestamps_dict[timestamps[ii]] = ii+1\n",
        "      df_total['timestep'] = df_total.apply(lambda x: timestamps_dict[x.time] + ((x.day-1)*96), axis=1 )\n",
        "      df_total = df_total.drop(columns=['timestamp','geohash6'])\n",
        "      df_total.to_csv('{}_transform.csv'.format(csvfile.split('.')[0]),index=False)\n",
        "  else:\n",
        "      df_total = pd.read_csv('{}_transform.csv'.format(csvfile.split('.')[0]))\n",
        "  return df_total\n",
        "\n",
        "df_train = decode_csv('training.csv')\n",
        "df_test = decode_csv('test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL-8lyQdtEdB",
        "colab_type": "text"
      },
      "source": [
        "First I have created the function of decode_csv. This function uses geohash2 library to obtain the exact number of latitude and longitude value with their respective error rate. I added 2 column to the imported data frame and I have also added a **time** column by converting the timestamp column into timestep data type in order for me to sort based on **day** and **time**. The reason behind this  is because data in the original **timestamp** column are read as string datatype. The function also save the transformed data frame as file to minimize the time taken to reload and applying the function to original csvfile.\n",
        "\n",
        "Below are the details of transformed dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AvqtyBVqMfb",
        "colab_type": "code",
        "outputId": "0591ac4a-a255-4dcb-e4e6-99b0061b5ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        }
      },
      "source": [
        "print(df_train.info())\n",
        "print(\"number of unique latitude - \",len(df_train['latitude'].unique().tolist()))\n",
        "print(\"number of unique longitude - \",len(df_train['longitude'].unique().tolist()))\n",
        "print(\"number of unique day - \",len(df_train['day'].unique().tolist()))\n",
        "print(\"number of unique timestep - \",len(df_train['timestep'].unique().tolist()))\n",
        "print(df_train.describe(include='all'))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4206321 entries, 0 to 4206320\n",
            "Data columns (total 6 columns):\n",
            "day          int64\n",
            "demand       float64\n",
            "latitude     float64\n",
            "longitude    float64\n",
            "time         object\n",
            "timestep     int64\n",
            "dtypes: float64(3), int64(2), object(1)\n",
            "memory usage: 192.6+ MB\n",
            "None\n",
            "number of unique latitude -  46\n",
            "number of unique longitude -  36\n",
            "number of unique day -  61\n",
            "number of unique timestep -  5847\n",
            "                 day        demand  ...      time      timestep\n",
            "count   4.206321e+06  4.206321e+06  ...   4206321  4.206321e+06\n",
            "unique           NaN           NaN  ...        96           NaN\n",
            "top              NaN           NaN  ...  09:00:00           NaN\n",
            "freq             NaN           NaN  ...     57281           NaN\n",
            "mean    3.145299e+01  1.050907e-01  ...       NaN  2.965249e+03\n",
            "std     1.768278e+01  1.592655e-01  ...       NaN  1.697748e+03\n",
            "min     1.000000e+00  3.092217e-09  ...       NaN  1.000000e+00\n",
            "25%     1.600000e+01  1.867379e-02  ...       NaN  1.478000e+03\n",
            "50%     3.200000e+01  5.043463e-02  ...       NaN  3.011000e+03\n",
            "75%     4.700000e+01  1.208644e-01  ...       NaN  4.429000e+03\n",
            "max     6.100000e+01  1.000000e+00  ...       NaN  5.856000e+03\n",
            "\n",
            "[11 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi9tiarj1LJB",
        "colab_type": "text"
      },
      "source": [
        "After I have obtained the transformed training and test csv file, I proceed to model the problem as a spatiol-temporal problem. Spatiol temporal modelling is simply a set of measurements (features) that posses distinct spatial information ( posses x and y information) that we can observed the changes along time ( time-axis). This specific problem can be apply not only to traffic demand forecasting but also applies to video frame prediction, precipitation nowcasting problem etc.\n",
        "\n",
        "</br>To model the traffic demand forecasting problem into spatiotemporal problem, I have proceed to get unique value of each extracted latitude and longitude value. Then I sorted the list of latitude and longitude value and find the increment of each next value. I noticed that the latitude and longitude are separated equally.\n",
        "The result is there's  46 unique latitude value and 36 unique longitude value.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMOuUKob4fqW",
        "colab_type": "code",
        "outputId": "0e926595-0ab7-4bd3-f18b-fed619262ce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "lats = df_train.sort_values(by=['latitude']).latitude.unique()\n",
        "lats_inc = lats[1:] - lats[:-1]\n",
        "print(lats_inc)\n",
        "logts = df_train.sort_values(by=['longitude']).longitude.unique()\n",
        "logts_inc = logts[1:] - logts[:-1]\n",
        "print(logts_inc)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.00549316 0.00549316 0.00549316 0.00549316 0.00549316 0.00549316\n",
            " 0.00549316 0.00549316 0.00549316 0.00549316 0.00549316 0.00549316\n",
            " 0.00549316 0.00549316 0.00549316 0.00549316 0.00549316 0.00549316\n",
            " 0.00549316 0.00549316 0.00549316 0.00549316 0.00549316 0.00549316\n",
            " 0.00549316 0.00549316 0.00549316 0.00549316 0.00549316 0.00549316\n",
            " 0.00549316 0.00549316 0.00549316 0.00549316 0.00549316 0.00549316\n",
            " 0.00549316 0.00549316 0.00549316 0.00549316 0.00549316 0.00549316\n",
            " 0.00549316 0.00549316 0.00549316]\n",
            "[0.01098633 0.01098633 0.01098633 0.01098633 0.01098633 0.01098633\n",
            " 0.01098633 0.01098633 0.01098633 0.01098633 0.01098633 0.01098633\n",
            " 0.01098633 0.01098633 0.01098633 0.01098633 0.01098633 0.01098633\n",
            " 0.01098633 0.01098633 0.01098633 0.01098633 0.01098633 0.01098633\n",
            " 0.01098633 0.01098633 0.01098633 0.01098633 0.01098633 0.01098633\n",
            " 0.01098633 0.01098633 0.01098633 0.01098633 0.01098633]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mmU5BMO5PXk",
        "colab_type": "text"
      },
      "source": [
        "Then, I created a function get_raw_SPT. This function serves to create a grid of demand data in the form of rectangular grid, based on the unique number of latitude and longitude. Any nodes within the grid that's has no information are assume to be zero. This 3-Dimensional array of data is named RAW_SpatiolTemporal(SPT). \n",
        "\n",
        "The first step to populate the grid with the demand is to map each latitudes value and longitudes value to integer number starting from 1 to max(latitude or longitude). Then I proceed to loop through each unique timestep to populate the grid from the latitude and longitude mapping that I have done.\n",
        "\n",
        "In the end, I manage to extract a 3D array with the shape of  (timestep, latitude, longitude) of training data to work with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCIBfTbTwkBp",
        "colab_type": "code",
        "outputId": "ccf97166-52ee-4f2d-ba7d-0a68d776d02c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "def get_raw_SPT(df,typ):\n",
        "  \"\"\"\n",
        "  INPUT : transformed dataframe with location decoded and sorted by timestamp\n",
        "  OUTPUT : 3D array data containing demand data with spatial information (2D array) arranged along with the time\n",
        "  FUNCTION : populate 2D array with demand data from transformed csvfile by mapping longitude and latitude to x and y axis of 2D array data\n",
        "              and stacked them up along with what time they happened.\n",
        "  \"\"\"\n",
        "  latitudes = df.sort_values(by=['latitude']).latitude.unique().tolist()\n",
        "  latitudes_dict = dict()\n",
        "  for ii in range(len(latitudes)):\n",
        "      latitudes_dict[latitudes[ii]] = ii\n",
        "  longitudes = df.sort_values(by=['longitude']).longitude.unique().tolist()\n",
        "  longitudes_dict = dict()\n",
        "  for ii in range(len(longitudes)):\n",
        "      longitudes_dict[longitudes[ii]] = ii\n",
        "\n",
        "  if not(os.path.isfile('RAW_{}_NPY.npy'.format(typ))):  \n",
        "      data = []\n",
        "\n",
        "      for ii in range(len(df.timestep.unique().tolist())):\n",
        "          df_ts = df.loc[df['timestep'] == ii+1]\n",
        "          X = [latitudes_dict[i] for i in df_ts['latitude'].tolist()]\n",
        "          Y = [longitudes_dict[i] for i in df_ts['longitude'].tolist()]\n",
        "          Z = df_ts['demand'].tolist()\n",
        "          matrix = np.zeros([len(latitudes),len(longitudes)])\n",
        "          matrix[X,Y] = Z\n",
        "          matrix = matrix[newaxis,:,:]\n",
        "          if len(data) == 0:\n",
        "              data = matrix\n",
        "          else:\n",
        "              data = np.concatenate([data,matrix],axis=0)\n",
        "      np.save('RAW_{}_NPY.npy'.format(typ),data)\n",
        "  else:\n",
        "      data = np.load('RAW_{}_NPY.npy'.format(typ))\n",
        "  return data\n",
        "      \n",
        "train_npy = get_raw_SPT(df_train,'train')\n",
        "test_npy = get_raw_SPT(df_test,'test')\n",
        "print(train_npy.shape)\n",
        "print(test_npy.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5847, 46, 36)\n",
            "(1440, 46, 36)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRDQwe-2FMja",
        "colab_type": "text"
      },
      "source": [
        "Next, to generate data and target for deep learning training purposes, I decided to do time series feature extraction. There's several term I have used here:\n",
        "\n",
        "\n",
        "1.   train_frame/frame : how many time steps to be taken into consideration in generating 1 data point.\n",
        "2.   stride                        : number of time steps to move the train_frame to generate next data point.\n",
        "3.   window_observation : number of time steps to be considered when perform time series feature extraction ( mean, standard deviation and variance)\n",
        "4.   delta/ delta_delta : rate of change of demand and first derivative rate of change of demand.\n",
        "\n",
        "In the previously extracted raw_SPT data array, demands at each time step are presented in 2D array form which preserve the spatial information and arranged along with time axis.\n",
        "In this scenario, I have decided to use train_frame of 7 time steps to generate data point in order to predict the demand of 5 time steps ahead. </br>\n",
        "\n",
        "The feature extraction of data point starts with getting the rate of change of demand and first derivative of rate of change. I extracted 2D array demand at time equals to t-1 and t-2, and continue to use 2D array demand at time = t to minus with 2D array demand time equals to t-1 to obtain the rate of change and use the same method to get the first derivative of rate of change. </br>\n",
        "Next, to obtain statistical feature from applying aggregration function on certain window of observational ( moving statistical feature), parameters such as window_observation need to be set in order to determine how many times steps before to apply aggregation function to get the statistical feature. </br>\n",
        "Finally, all of 6 2D array will be concatenated together to create a data within 1 time step with 6 feature.</br>\n",
        "\n",
        "To complete the generation of data points for training, I proceed to do the feature extraction process mentioned above for 7 times for each time steps and concatenate them into 1 single data point with array shape of ( 7, 46, 36, 6) where 7 is for 7 timesteps, 46 and 36 are respective number for latitude and longitude and 6 is the demand and demand features that has been extracted.\n",
        "Each training point is then concatenate into 1 big npy array to feed into deep learning training process.</br>\n",
        "\n",
        "For target data extraction, it's more straightforward where 5 timestep 2D data are extracted because we want to predict what's demand like for next 5 timesteps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8W0mtmE7LLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def time_fv_extract(data,iteration,train_frame,stride,window_observation=3):\n",
        "    \"\"\"\n",
        "    INPUT : \n",
        "        data = raw_spationtemporal numpy array from previous function\n",
        "        iteration = i-th iteration of data point generation\n",
        "        train_frame = how many timesteps to be considered in 1 data points\n",
        "        stride = how many timesteps jump for generating another data point\n",
        "        window_observation = amount timesteps to be considered to apply statiscal aggregation function.\n",
        "    OUPUT :\n",
        "        feature extracted from raw_spatiotemporal data\n",
        "    FUNCTION : feature extraction for making data-target set\n",
        "    \"\"\"\n",
        "    _seg = data[iteration*stride:iteration*stride+train_frame,:,:]\n",
        "    _1seg = data[(iteration-1)*stride:(iteration-1)*stride+train_frame,:,:]\n",
        "    _2seg = data[(iteration-2)*stride:(iteration-2)*stride+train_frame,:,:]\n",
        "    _delta = _seg-_1seg\n",
        "    _1delta = _1seg-_2seg\n",
        "    _deltadelta = _delta-_1delta\n",
        "    _stats_seg = []\n",
        "    for xx in range(train_frame):\n",
        "        _forStats =  data[iteration*stride+xx-window_observation+1:iteration*stride+xx+1,:,:]\n",
        "        _mean = np.mean(_forStats,axis=0)\n",
        "        _std = np.std(_forStats,axis=0)\n",
        "        _var = np.var(_forStats,axis=0)\n",
        "        _mean = _mean[:,:,newaxis]\n",
        "        _std = _std[:,:,newaxis]\n",
        "        _var = _var[:,:,newaxis]\n",
        "        _stats = np.concatenate([_mean,_std,_var],axis=-1)\n",
        "        _stats = _stats[newaxis,:,:,:]\n",
        "        if len(_stats_seg) == 0:\n",
        "            _stats_seg = _stats\n",
        "        else:\n",
        "            _stats_seg = np.concatenate([_stats_seg,_stats],axis=0)\n",
        "    _seg = _seg[:,:,:,newaxis]\n",
        "    _delta = _delta[:,:,:,newaxis]\n",
        "    _deltadelta = _deltadelta[:,:,:,newaxis]\n",
        "    _res = np.concatenate([_seg,_delta,_deltadelta,_stats_seg],axis=-1)\n",
        "    return _res\n",
        "\n",
        "def data_gen(data,typ,train_frame=1344,target_frame=5,stride=5,):\n",
        "    \"\"\"\n",
        "    INPUT : \n",
        "        data = raw_spationtemporal numpy array from previous function\n",
        "        typ = for naming, whether \"train\" or \"test\"\n",
        "        train_frame = how many timesteps to be considered in 1 data points\n",
        "        stride = how many timesteps jump for generating another data point\n",
        "    OUTPUT : \n",
        "        data and target set used for deep learning model.\n",
        "    FUNCTION : concatenated extracted feature into a data target set for deep learning model\n",
        "    \"\"\"\n",
        "    if not(os.path.isfile(\"{}_data_frame-{}_stride-{}.npy\".format(typ,train_frame,stride)) and os.path.isfile(\"{}_target_frame-{}_stride-{}.npy\".format(typ,train_frame,stride))):\n",
        "        dt_points  = floor((data.shape[0]-train_frame)/stride)-5\n",
        "        _data = []\n",
        "        _target = []\n",
        "        print(\"Number of {} Sample Generated - \".format(typ),dt_points)\n",
        "        for ii in tqdm(range(2,dt_points)):\n",
        "            _train_segment = time_fv_extract(data,ii,train_frame,stride)\n",
        "            _target_segment = data[ii*stride+train_frame:ii*stride+train_frame+target_frame,:,:]\n",
        "            _train_segment = _train_segment[newaxis,:,:,:,:]\n",
        "            _target_segment = _target_segment[newaxis,:,:,:]\n",
        "            if len(_data) == 0:\n",
        "                _data = _train_segment\n",
        "                _target = _target_segment\n",
        "            else:\n",
        "                _data = np.concatenate([_data,_train_segment],axis=0)\n",
        "                _target = np.concatenate([_target,_target_segment],axis=0)\n",
        "        np.save(\"{}_data_frame-{}_stride-{}.npy\".format(typ,train_frame,stride),_data)\n",
        "        np.save(\"{}_target_frame-{}_stride-{}.npy\".format(typ,train_frame,stride),_target)\n",
        "    else:\n",
        "        _data = np.load(\"{}_data_frame-{}_stride-{}.npy\".format(typ,train_frame,stride))\n",
        "        _target = np.load(\"{}_target_frame-{}_stride-{}.npy\".format(typ,train_frame,stride))\n",
        "    return _data,_target\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_DnHzNY7xRf",
        "colab_type": "code",
        "outputId": "ff2f358d-ea50-48df-bc7d-8fd2a1afbde3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "source": [
        "train_data, train_target = data_gen(train_npy,'train',train_frame=7,target_frame=5,stride=1)\n",
        "test_data,test_target = data_gen(test_npy,'test',train_frame=7,target_frame=5,stride=1)\n",
        "print('Train Data set train - {} target - {}.'.format(train_data.shape,train_target.shape))\n",
        "print('Test Data set train - {} target - {}.'.format(test_data.shape,test_target.shape))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 30/5833 [00:00<00:19, 292.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of train Sample Generated -  5835\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5833/5833 [56:27<00:00,  1.15s/it]\n",
            "  2%|▏         | 23/1426 [00:00<00:06, 223.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of test Sample Generated -  1428\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1426/1426 [03:25<00:00,  3.49it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Data set train - (5833, 7, 46, 36, 6) target - (5833, 5, 46, 36).\n",
            "Test Data set train - (1426, 7, 46, 36, 6) target - (1426, 5, 46, 36).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpnnm9YoSH8k",
        "colab_type": "text"
      },
      "source": [
        "Some specific parameters setting to be inserted within the model formation are done here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLoSEhQR7xIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_FRAME = 7\n",
        "N_CHANNEL = 6\n",
        "LATITUDE = 46\n",
        "LONGITUDE = 36\n",
        "STRIDE = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqkHqVUBeEj_",
        "colab_type": "text"
      },
      "source": [
        "get_tt_data function is created to load the npy dataset for deep learning model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eagQOyQByJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tt_data(trainframe=1344,stride=5,folder='train'):\n",
        "    \"\"\"\n",
        "    INPUT : \n",
        "        train_frame = how many timesteps to be considered in 1 data points\n",
        "        stride =  how many timesteps jump for generating another data point\n",
        "        folder = data type to be extracted ( training or test)\n",
        "    OUTPUT :\n",
        "        data and target set loaded as numpy array\n",
        "    FUNCTION : To load generated dataset\n",
        "    \"\"\"\n",
        "    tr_dt = np.load(\"{}_data_frame-{}_stride-{}.npy\".format(folder,trainframe,stride))\n",
        "    tg_dt = np.load(\"{}_target_frame-{}_stride-{}.npy\".format(folder,trainframe,stride))\n",
        "    print(\"{} data shape - {}\".format(folder,tr_dt.shape))\n",
        "    print(\"{} target shape - {}\".format(folder,tg_dt.shape))\n",
        "    return tr_dt, tg_dt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvwBv_SfhfCJ",
        "colab_type": "text"
      },
      "source": [
        "After frame the problem into a spatiotemporal problem, I have done several research on which deep learning model works well with predicting the problem. The model is based on ConvLSTM that has been explained here (https://medium.com/neuronio/an-introduction-to-convlstm-55c9025563a7). In summary, In addition of using sigmoid and tanh within LSTM cell, Convolutional operations are used. </br>![alt text](https://cdn-images-1.medium.com/max/1000/1*u8neecA4w6b_F1NgnyPP0Q.png)  </br>\n",
        "I have drawn inspiration from [Shi et al](https://papers.nips.cc/paper/5955-convolutional-lstm-network-a-machine-learning-approach-for-precipitation-nowcasting.pdf) with their application of ConvLSTM on precipitation forecasting. With some modification, I have constucted the model to be able to train and predict precipitation rate for 5 time step ahead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1kfYHsqBRk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convlstm_model3(train_frame,n_channel,n_latitude,n_longitude):\n",
        "    \"\"\"\n",
        "    INPUT :\n",
        "        train_frame = how many timesteps to be considered in 1 data points\n",
        "        n_channel = how many features within 1 timestep is extracted\n",
        "        n_latitude = unique number of latitude\n",
        "        n_longitude = unique number of longitude\n",
        "    OUTPUT :\n",
        "        ConvLSTM model created based on the settings specified in the input\n",
        "    FUNCTION : Create structure of ConvLSTM model to solve spatiotemporal problem\n",
        "    \"\"\"\n",
        "    input_layer = Input(batch_shape=(None, train_frame, n_latitude,n_longitude, n_channel))\n",
        "\n",
        "    x = ConvLSTM2D(64, (3,3), strides=(1,1), padding='same', return_sequences=True)(input_layer)\n",
        "\n",
        "    x1 = ConvLSTM2D(16, (3,3), return_sequences=True, padding='same', name='x1_layer_1')(x)\n",
        "    #x1 = ConvLSTM2D(64, (3,3), return_sequences=True, padding='same', name='x1_layer_2')(x1)\n",
        "    #x1 = ConvLSTM2D(1, (3,3), padding='same', name='x1_layer')(x)\n",
        "    x1 = ConvLSTM2D(1, (1,1), name='x1_layer_2')(x1)\n",
        "    x1 = Reshape((1, n_latitude, n_longitude))(x1)\n",
        "\n",
        "    x2 = ConvLSTM2D(16, (3,3), return_sequences=True, padding='same', name='x2_layer_1')(x)\n",
        "    #x2 = ConvLSTM2D(64, (3,3), return_sequences=True, padding='same', name='x2_layer_2')(x2)\n",
        "    #x2 = ConvLSTM2D(1, (3,3), padding='same', name='x2_layer')(x)\n",
        "    x2 = ConvLSTM2D(1, (1,1), name='x2_layer_2')(x2)\n",
        "    x2 = Reshape((1, n_latitude, n_longitude))(x2)\n",
        "    \n",
        "    x3 = ConvLSTM2D(16, (3,3), return_sequences=True, padding='same', name='x3_layer_1')(x)\n",
        "    #x3 = ConvLSTM2D(64, (3,3), return_sequences=True, padding='same', name='x3_layer_2')(x3)\n",
        "    #x3 = ConvLSTM2D(1, (3,3), padding='same', name='x3_layer')(x)\n",
        "    x3 = ConvLSTM2D(1, (1,1), name='x3_layer_2')(x3)\n",
        "    x3 = Reshape((1, n_latitude, n_longitude))(x3)\n",
        "    \n",
        "    x4 = ConvLSTM2D(16, (3,3), return_sequences=True, padding='same', name='x4_layer_1')(x)\n",
        "    #x4 = ConvLSTM2D(64, (3,3), return_sequences=True, padding='same', name='x4_layer_2')(x4)\n",
        "    #x4 = ConvLSTM2D(1, (3,3), padding='same', name='x4_layer')(x)\n",
        "    x4 = ConvLSTM2D(1, (1,1), name='x4_layer_2')(x4)\n",
        "    x4 = Reshape((1, n_latitude, n_longitude))(x4)\n",
        "    \n",
        "    x5 = ConvLSTM2D(16, (3,3), return_sequences=True, padding='same', name='x5_layer_1')(x)\n",
        "    #x5 = ConvLSTM2D(64, (3,3), return_sequences=True, padding='same', name='x5_layer_2')(x5)\n",
        "    #x5 = ConvLSTM2D(1, (3,3), padding='same', name='x5_layer')(x)\n",
        "    x5 = ConvLSTM2D(1, (1,1), name='x5_layer_2')(x5)\n",
        "    x5 = Reshape((1, n_latitude, n_longitude))(x5)\n",
        "    \n",
        "\n",
        "    output = Concatenate(name='concat_layer', axis=1)([x1, x2, x3, x4, x5])\n",
        "    #output = Concatenate(name='concat_layer', axis=1)([x1, x2, x3])\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LupYpUpB04d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model_name='ConvLSTM',opt='Adam'):\n",
        "    \"\"\"\n",
        "    INPUT : optimizer function for deep learning model\n",
        "    FUNCTION : wrapper function to start for training process\n",
        "    \"\"\"\n",
        "    train, target = get_tt_data(TRAIN_FRAME,STRIDE)\n",
        "    if model_name == \"GoogleCongestion\":\n",
        "        model = ConvLSTM_GoogleCongestion(TRAIN_FRAME,N_CHANNEL,LATITUDE,LONGITUDE)\n",
        "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001), metrics=['acc'])\n",
        "    elif model_name == 'ConvLSTM':\n",
        "        model = convlstm_model3(TRAIN_FRAME,N_CHANNEL,LATITUDE,LONGITUDE)\n",
        "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001), metrics=['acc'])\n",
        "    model.fit(train,target,batch_size=32, epochs=10, verbose=1, validation_split = 0.2)\n",
        "\n",
        "    model.save(\"{}_TRIAL5_tf-{}_sd-{}_{}.h5\".format(model_name,TRAIN_FRAME,STRIDE,opt))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBK6siLARBeQ",
        "colab_type": "text"
      },
      "source": [
        "Proceed to train deep learning model by calling the train_model function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1TrPqn6CGJ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1458
        },
        "outputId": "3f02ac06-e2e4-4925-9ac8-0c7b97fe9f03"
      },
      "source": [
        "train_model(model='ConvLSTM')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0617 15:49:58.439812 140675626592128 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0617 15:49:58.593121 140675626592128 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0617 15:49:58.610487 140675626592128 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train data shape - (5833, 7, 46, 36, 6)\n",
            "train target shape - (5833, 5, 46, 36)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0617 15:50:00.951709 140675626592128 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 7, 46, 36, 6) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_lst_m2d_1 (ConvLSTM2D)     (None, 7, 46, 36, 64 161536      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "x1_layer_1 (ConvLSTM2D)         (None, 7, 46, 36, 16 46144       conv_lst_m2d_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "x2_layer_1 (ConvLSTM2D)         (None, 7, 46, 36, 16 46144       conv_lst_m2d_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "x3_layer_1 (ConvLSTM2D)         (None, 7, 46, 36, 16 46144       conv_lst_m2d_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "x4_layer_1 (ConvLSTM2D)         (None, 7, 46, 36, 16 46144       conv_lst_m2d_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "x5_layer_1 (ConvLSTM2D)         (None, 7, 46, 36, 16 46144       conv_lst_m2d_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "x1_layer_2 (ConvLSTM2D)         (None, 46, 36, 1)    72          x1_layer_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "x2_layer_2 (ConvLSTM2D)         (None, 46, 36, 1)    72          x2_layer_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "x3_layer_2 (ConvLSTM2D)         (None, 46, 36, 1)    72          x3_layer_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "x4_layer_2 (ConvLSTM2D)         (None, 46, 36, 1)    72          x4_layer_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "x5_layer_2 (ConvLSTM2D)         (None, 46, 36, 1)    72          x5_layer_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 1, 46, 36)    0           x1_layer_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 1, 46, 36)    0           x2_layer_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 1, 46, 36)    0           x3_layer_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_4 (Reshape)             (None, 1, 46, 36)    0           x4_layer_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_5 (Reshape)             (None, 1, 46, 36)    0           x5_layer_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concat_layer (Concatenate)      (None, 5, 46, 36)    0           reshape_1[0][0]                  \n",
            "                                                                 reshape_2[0][0]                  \n",
            "                                                                 reshape_3[0][0]                  \n",
            "                                                                 reshape_4[0][0]                  \n",
            "                                                                 reshape_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 392,616\n",
            "Trainable params: 392,616\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0617 15:50:01.734393 140675626592128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0617 15:50:07.696327 140675626592128 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "W0617 15:50:08.064543 140675626592128 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 4666 samples, validate on 1167 samples\n",
            "Epoch 1/10\n",
            "4666/4666 [==============================] - 137s 29ms/step - loss: 0.0022 - acc: 0.6060 - val_loss: 0.0014 - val_acc: 0.6430\n",
            "Epoch 2/10\n",
            "4666/4666 [==============================] - 127s 27ms/step - loss: 0.0011 - acc: 0.6476 - val_loss: 0.0012 - val_acc: 0.6629\n",
            "Epoch 3/10\n",
            "4666/4666 [==============================] - 127s 27ms/step - loss: 9.3665e-04 - acc: 0.6591 - val_loss: 0.0010 - val_acc: 0.6637\n",
            "Epoch 4/10\n",
            "4666/4666 [==============================] - 127s 27ms/step - loss: 9.0251e-04 - acc: 0.6640 - val_loss: 0.0010 - val_acc: 0.6751\n",
            "Epoch 5/10\n",
            "4666/4666 [==============================] - 127s 27ms/step - loss: 8.7530e-04 - acc: 0.6698 - val_loss: 0.0010 - val_acc: 0.6715\n",
            "Epoch 6/10\n",
            "4666/4666 [==============================] - 127s 27ms/step - loss: 8.5888e-04 - acc: 0.6707 - val_loss: 9.8717e-04 - val_acc: 0.6764\n",
            "Epoch 7/10\n",
            "4666/4666 [==============================] - 127s 27ms/step - loss: 8.5684e-04 - acc: 0.6737 - val_loss: 9.7987e-04 - val_acc: 0.6780\n",
            "Epoch 8/10\n",
            "4666/4666 [==============================] - 127s 27ms/step - loss: 8.3417e-04 - acc: 0.6731 - val_loss: 9.7003e-04 - val_acc: 0.6738\n",
            "Epoch 9/10\n",
            "4666/4666 [==============================] - 127s 27ms/step - loss: 8.2391e-04 - acc: 0.6734 - val_loss: 9.6198e-04 - val_acc: 0.6767\n",
            "Epoch 10/10\n",
            "4666/4666 [==============================] - 127s 27ms/step - loss: 8.1864e-04 - acc: 0.6719 - val_loss: 9.8579e-04 - val_acc: 0.6675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk9ytzUpRSxY",
        "colab_type": "text"
      },
      "source": [
        "Below is the metrics to see how good is the predictive ability of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33R_ntOQKSB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def smape(A, F):\n",
        "    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n",
        "\n",
        "def evaluate_model(model='ConvLSTM',opt='Adam'):\n",
        "    \"\"\"\n",
        "    FUNCTION : to evaluate the model ability to predict using SMAPE\n",
        "    \"\"\"\n",
        "    if model == 'ConvLSTM':\n",
        "        model = load_model(\"{}_TRIAL5_tf-{}_sd-{}_{}.h5\".format(model,TRAIN_FRAME,STRIDE,opt))\n",
        "    elif model == 'GoogleCongestion':\n",
        "        model = load_model(\"{}_TRIAL5_tf-{}_sd-{}_{}.h5\".format(model,TRAIN_FRAME,STRIDE,opt))\n",
        "    test_data, test_target = get_tt_data(TRAIN_FRAME,STRIDE,folder='test')\n",
        "    pred = model.predict(test_data)\n",
        "    pred = pred.reshape(pred.shape[0],pred.shape[1],pred.shape[2]*pred.shape[3])\n",
        "    test_target = test_target.reshape(test_target.shape[0],test_target.shape[1],test_target.shape[2]*test_target.shape[3])\n",
        "    smape_seg = 0\n",
        "    rmse_seg = 0\n",
        "    metrics = []\n",
        "    five_metrics = []\n",
        "    for ii in range(pred.shape[0]):\n",
        "        five_smape = 0\n",
        "        five_rmse = 0\n",
        "        for jj in range(pred.shape[1]):\n",
        "            pred_pertimestep = pred[ii,jj,:]\n",
        "            test_targetpertimestep = test_target[ii,jj,:]\n",
        "            smape_seg = smape(test_targetpertimestep,pred_pertimestep)\n",
        "            five_smape = smape(test_targetpertimestep,pred_pertimestep) + five_smape\n",
        "            rmse_seg = np.sqrt(mean_squared_error(test_targetpertimestep,pred_pertimestep))\n",
        "            five_rmse = np.sqrt(mean_squared_error(test_targetpertimestep,pred_pertimestep)) + five_rmse\n",
        "            metrics.append((smape_seg,rmse_seg))\n",
        "        five_metrics.append((five_smape,five_rmse))\n",
        "    five_metrics = np.asarray(five_metrics)\n",
        "    metrics = np.asarray(metrics)\n",
        "    print(\"Average Symmetric Mean Absolute Percentage Error of prediction is (per 5 timestep)\",np.mean(five_metrics,axis=0)[0])\n",
        "    print(\"Average Root Mean Squared Error of prediction is (per 5 timestep)\",np.mean(five_metrics,axis=0)[1])\n",
        "    \n",
        "    print(\"Average Symmetric Mean Absolute Percentage Error of prediction is (per timestep)\",np.mean(metrics,axis=0)[0])\n",
        "    print(\"Average Root Mean Squared Error of prediction is (per timestep)\",np.mean(metrics,axis=0)[1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1WDv_5dVDXH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "bf7e9518-9863-4793-ef1b-f54d6e739b7e"
      },
      "source": [
        "evaluate_model()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test data shape - (1426, 7, 46, 36, 6)\n",
            "test target shape - (1426, 5, 46, 36)\n",
            "Average Symmetric Mean Absolute Percentage Error of prediction is (per 5 timestep) 672.2149981587627\n",
            "Average Root Mean Squared Error of prediction is (per 5 timestep) 0.14479674231288528\n",
            "Average Symmetric Mean Absolute Percentage Error of prediction is (per timestep) 134.44299963175246\n",
            "Average Root Mean Squared Error of prediction is (per timestep) 0.0289593484625771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2zQnhpNLLeO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ConvLSTM_GoogleCongestion(train_frame,n_channel,n_latitude,n_longitude):\n",
        "    input_layer = Input(batch_shape=(None, train_frame, n_latitude,n_longitude, n_channel))\n",
        "    x = ConvLSTM2D(40, (3,3), strides=(1,1), padding='same', activation= 'relu', return_sequences=True, name='1_ConvLSTM')(input_layer)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ConvLSTM2D(40, (3,3), strides=(1,1), padding='same', activation= 'relu', return_sequences=True, name='2_ConvLSTM')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ConvLSTM2D(40, (3,3), strides=(1,1), padding='same', activation= 'relu', return_sequences=True, name='3_ConvLSTM')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ConvLSTM2D(40, (3,3), strides=(1,1), padding='same', activation= 'relu', return_sequences=True, name='4_ConvLSTM')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv3D(5,(3,3,3),padding='same',data_format='channels_first', name='1_3DConv')(x)\n",
        "    x = Conv3D(1,(3,3,3),padding='same',data_format='channels_last', name='2_3DConv')(x)\n",
        "    output = Reshape((5,n_latitude,n_longitude)) (x)\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuNP711CROyG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        },
        "outputId": "8a94d6d7-317b-42a1-c2bc-07d19a7e84cd"
      },
      "source": [
        "train_model(model='GoogleCongestion')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data shape - (5833, 7, 46, 36, 6)\n",
            "train target shape - (5833, 5, 46, 36)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 7, 46, 36, 6)      0         \n",
            "_________________________________________________________________\n",
            "1_ConvLSTM (ConvLSTM2D)      (None, 7, 46, 36, 40)     66400     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 7, 46, 36, 40)     160       \n",
            "_________________________________________________________________\n",
            "2_ConvLSTM (ConvLSTM2D)      (None, 7, 46, 36, 40)     115360    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 7, 46, 36, 40)     160       \n",
            "_________________________________________________________________\n",
            "3_ConvLSTM (ConvLSTM2D)      (None, 7, 46, 36, 40)     115360    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 7, 46, 36, 40)     160       \n",
            "_________________________________________________________________\n",
            "4_ConvLSTM (ConvLSTM2D)      (None, 7, 46, 36, 40)     115360    \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 7, 46, 36, 40)     160       \n",
            "_________________________________________________________________\n",
            "1_3DConv (Conv3D)            (None, 5, 46, 36, 40)     950       \n",
            "_________________________________________________________________\n",
            "2_3DConv (Conv3D)            (None, 5, 46, 36, 1)      1081      \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 5, 46, 36)         0         \n",
            "=================================================================\n",
            "Total params: 415,151\n",
            "Trainable params: 414,831\n",
            "Non-trainable params: 320\n",
            "_________________________________________________________________\n",
            "Train on 4666 samples, validate on 1167 samples\n",
            "Epoch 1/10\n",
            "4666/4666 [==============================] - 127s 27ms/step - loss: 0.0697 - acc: 0.3050 - val_loss: 0.0196 - val_acc: 0.3916\n",
            "Epoch 2/10\n",
            "4666/4666 [==============================] - 122s 26ms/step - loss: 0.0045 - acc: 0.4787 - val_loss: 0.0082 - val_acc: 0.4822\n",
            "Epoch 3/10\n",
            "4666/4666 [==============================] - 122s 26ms/step - loss: 0.0031 - acc: 0.5289 - val_loss: 0.0053 - val_acc: 0.5313\n",
            "Epoch 4/10\n",
            "4666/4666 [==============================] - 122s 26ms/step - loss: 0.0024 - acc: 0.5575 - val_loss: 0.0038 - val_acc: 0.5572\n",
            "Epoch 5/10\n",
            "4666/4666 [==============================] - 122s 26ms/step - loss: 0.0020 - acc: 0.5761 - val_loss: 0.0032 - val_acc: 0.5772\n",
            "Epoch 6/10\n",
            "4666/4666 [==============================] - 122s 26ms/step - loss: 0.0018 - acc: 0.5887 - val_loss: 0.0026 - val_acc: 0.5945\n",
            "Epoch 7/10\n",
            "4666/4666 [==============================] - 122s 26ms/step - loss: 0.0017 - acc: 0.5975 - val_loss: 0.0021 - val_acc: 0.6016\n",
            "Epoch 8/10\n",
            "4666/4666 [==============================] - 123s 26ms/step - loss: 0.0015 - acc: 0.6055 - val_loss: 0.0020 - val_acc: 0.6086\n",
            "Epoch 9/10\n",
            "4666/4666 [==============================] - 122s 26ms/step - loss: 0.0014 - acc: 0.6115 - val_loss: 0.0018 - val_acc: 0.6161\n",
            "Epoch 10/10\n",
            "4666/4666 [==============================] - 123s 26ms/step - loss: 0.0014 - acc: 0.6166 - val_loss: 0.0016 - val_acc: 0.6224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXNvW0xXWD0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}